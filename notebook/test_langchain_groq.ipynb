{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\", \"あなたは役立つAIアシスタントです。\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\", \"あなたについて教えてください。\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model_name=\"llama-3.3-70b-versatile\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは親切なAIアシスタントです。\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"user001\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = with_message_history.invoke(\n",
    "    {\"input\": \"こんにちは！私の名前は太郎です。\"},\n",
    "    config=config\n",
    ")\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = with_message_history.invoke(\n",
    "    {\"input\": \"私の名前を覚えていますか？\"},\n",
    "    config=config\n",
    ")\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 履歴の確認\n",
    "history = get_session_history(\"user001\")\n",
    "for message in history.messages:\n",
    "    print(f\"{message.type}: {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"\"\"Groqとは\n",
    "Groq は、超低レイテンシかつ高スループットの AI 推論専用プロセッサ（LPU: Language Processing Unit） を開発している米国の企業である。特に大規模言語モデル (LLM) や生成 AI の推論に特化しており、従来の GPU や CPU とは異なるアーキテクチャを採用している。\"\"\",\n",
    "    \"\"\"特徴\n",
    "1. アーキテクチャ設計\n",
    "  - 単一命令ストリーム：Groq のチップは GPU のように多数のスレッドを並列に実行するのではなく、単一の制御フローで大量の演算を同期的に処理する方式を採用している。\n",
    "  - 行列演算に最適化：ディープラーニングのコアとなる行列・ベクトル演算を最適化したハードウェア設計である。\n",
    "  - 固定パイプライン：メモリ帯域やキャッシュ制御のオーバーヘッドを排除し、決まったパターンの計算を極めて効率よく処理する。\n",
    "2. 性能\n",
    "  - 超低レイテンシ：数ミリ秒以下での推論応答を実現可能であり、GPU ベースの処理と比べて遅延が大幅に小さい。\n",
    "  - 高スループット：並列推論リクエストを処理する能力に優れており、特にリアルタイム応答が必要な LLM サービスに適している。\n",
    "  - 消費電力あたりの効率：GPU に比べて推論性能あたりのエネルギー効率が高いとされる。\n",
    "3. ソフトウェアスタック\n",
    "  - GroqWare：専用のコンパイラやランタイム環境を提供し、TensorFlow / PyTorch などの既存 ML フレームワークからモデルを Groq LPU に移植可能にしている。\n",
    "  - 推論最適化：モデルを LPU 向けに変換し、最大限の効率を引き出す。\"\"\",\n",
    "    \"\"\"利用分野\n",
    "  - 大規模言語モデル (LLM) 推論：ChatGPT のような会話型 AI の高速応答\n",
    "  - 金融分野：リアルタイムのリスク評価やアルゴ取引\n",
    "  - 自動運転・ロボティクス：ミリ秒単位での意思決定が必要な制御システム\n",
    "  - クラウド AI サービス：低レイテンシな API 提供\"\"\",\n",
    "    \"\"\"GPU / TPU との比較\n",
    "  - GPU：汎用的だが、スレッド制御やキャッシュ管理のオーバーヘッドがある。\n",
    "  - TPU：Google 専用でクラウド特化。行列演算に強いが、柔軟性に欠ける。\n",
    "  - Groq LPU：推論専用。構造を単純化することで遅延を極限まで削減している。\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.create_documents(docs)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "vector = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "以下のコンテキスト情報のみを使用して、質問に答えてください。\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "document_chain = create_stuff_documents_chain(model, rag_prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = retrieval_chain.invoke({\"input\": \"Groqとは何ですか？\"})\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"context\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG + memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"あなたは厳格な教授です。質問に対して厳しめに回答します。\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    model, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"\n",
    "以下のコンテキスト情報のみを使用して、質問に答えてください。\n",
    "コンテキスト情報から答えがわからない場合は、正直にそう言ってください。\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "rag_with_history_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_with_history_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "config_rag = {\"configurable\": {\"session_id\": \"rag_user002\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"こんにちは、私は新入生の圭一です。Groqとは何ですか？\"},\n",
    "    config=config_rag\n",
    ")\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"そのソフトウェアスタックについて教えてください。\"},\n",
    "    config=config_rag\n",
    ")\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"どのような分野で活躍しますか？\"},\n",
    "    config=config_rag\n",
    ")\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"ところで、私の名前を覚えていますか？\"},\n",
    "    config=config_rag\n",
    ")\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"あなたは？\"},\n",
    "    config=config_rag\n",
    ")\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"あれ？教授では？\"},\n",
    "    config=config_rag\n",
    ")\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-paper-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
